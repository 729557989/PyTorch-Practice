-> 若遇到梯度消失或梯度爆炸则：
1. 替换易训练神经元
    Sigmoid -> Leaky ReLU
    tanh -> ReLU
2. 改进梯度优化算法：
    使用ADAM
3. 使用batch normalization

-> 梯度爆炸和梯度消失：
1. 梯度消失：梯度太小，无法进行参数的更新，梯度小到数据类型无法表示
2. 梯度爆炸： 梯度太大，梯度大到数据类型无法表示，出现Nan

-> 该如何使用pytorch 的 batchnorm
如果input数据是二维的话就用batchnorm1d， 若是三维则用batchnorm2d

-> Passing in dim=-1 applies softmax to the last dimension. So, after you do this, the elements of the last dimension will sum to 1.

# check if pytorch is using GPU:
# 1. go to CMD``
# 2. enter: nvidia-smi
# 3. see if GPU was used, don't trust the stupid TASK MANAGER